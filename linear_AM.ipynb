{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=7\n",
    "\n",
    "import jax\n",
    "from jax import nn\n",
    "from jax import numpy as jnp\n",
    "from jax import random as jr\n",
    "from util import *\n",
    "import optax\n",
    "from flax.traverse_util import flatten_dict, unflatten_dict\n",
    "\n",
    "import pickle\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear(nn.Module):\n",
    "    d: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        z = nn.Dense(self.d, use_bias = False, name = 'layer1')(x)\n",
    "        return z\n",
    "    \n",
    "def run(d, N, lr, probs, rng):\n",
    "    \n",
    "    lr = lr\n",
    "    wd = 0.\n",
    "    steps = 2**12\n",
    "    save_every = steps // 128\n",
    "    \n",
    "    U = jr.normal(rng.next(), (N, d))\n",
    "    U = vmap(lambda u: u/jnp.linalg.norm(u))(U)\n",
    "    E = jr.normal(rng.next(), (N, d))\n",
    "    E = vmap(lambda u: u/jnp.linalg.norm(u))(E)\n",
    "    \n",
    "    model = linear(d = d)\n",
    "\n",
    "    p0 = model.init(rng.next(), E)\n",
    "    p0 = flatten_dict(p0[\"params\"], sep=\".\")\n",
    "    \n",
    "    \n",
    "    @partial(jit, static_argnames=\"mutable\")\n",
    "    def f(p, *args, **kwargs):\n",
    "        p = dict(params=unflatten_dict(p, sep=\".\"))\n",
    "        return model.apply(p, *args, **kwargs)\n",
    "    @jit\n",
    "    def criterion(i, y_hat):\n",
    "        return -jnp.log(y_hat[i])\n",
    "\n",
    "    @jit\n",
    "    def loss_fn(p):\n",
    "        preds = nn.softmax(f(p, E) @ U.T)\n",
    "        return vmap(criterion)(jnp.arange(N), preds) @ probs\n",
    "\n",
    "    @jit\n",
    "    def accuracy(p):\n",
    "        preds = f(p, E) @ U.T\n",
    "        return vmap(lambda yi, fi: jnp.argmax(fi) == yi)(jnp.arange(N), preds) @ probs\n",
    "    \n",
    "    @jit\n",
    "    def linear_AM_acc():\n",
    "        preds = E @ E.T @ U @ U.T\n",
    "        return vmap(lambda yi, fi: jnp.argmax(fi) == yi)(jnp.arange(N), preds) @ probs\n",
    "    \n",
    "    def custom_schedule(step):\n",
    "        return jnp.where(step < 2**12, lr, lr/10.)\n",
    "\n",
    "    # schedule = optax.Schedule(custom_schedule)\n",
    "    \n",
    "    # Init optimizer\n",
    "    opt = optax.adamw(learning_rate = lr, weight_decay=wd)\n",
    "    # opt = optax.sgd(learning_rate = lr, momentum = 0.9)\n",
    "\n",
    "    @jit\n",
    "    def step_fn_adam(p, opt_state):\n",
    "        loss, g = jax.value_and_grad(loss_fn)(p)\n",
    "        updates, opt_state = opt.update(g, opt_state, p)\n",
    "        p = optax.apply_updates(p, updates)\n",
    "        return p, opt_state, loss\n",
    "    \n",
    "    p = p0\n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    opt_state = opt.init(p0)\n",
    "    for i in trange(steps):\n",
    "        p, opt_state, loss = step_fn_adam(p, opt_state)\n",
    "        losses.append(loss)\n",
    "        if i % save_every == 0:\n",
    "            acc = accuracy(p)\n",
    "            accs.append(acc)\n",
    "            if acc > 0.99:\n",
    "                break\n",
    "\n",
    "    acc = max(accs)\n",
    "    loss = min(losses)\n",
    "    # AM_acc = linear_AM_acc()\n",
    "    AM_acc = None\n",
    "    print(acc, AM_acc, loss)\n",
    "    return acc, losses, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_sweep(d, min_N, max_N, lrs, rng):\n",
    "    \n",
    "    max_size = max_N\n",
    "    min_size = min_N\n",
    "    threshhold = 0.99\n",
    "    while (max_size/min_size > 1.01) and (max_size - min_size > 1):\n",
    "        N = int(0.5*(max_size + min_size))\n",
    "        \n",
    "        for lr in lrs:\n",
    "            acc, _, loss = run(d, N, lr, jnp.ones(N)/N, rng)\n",
    "            print(N, lr, acc)\n",
    "            if acc >= threshhold:\n",
    "                min_size = N\n",
    "                break\n",
    "        if acc < threshhold:\n",
    "            max_size = N\n",
    "            \n",
    "    return min_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_range = [int(2**i) for i in jnp.arange(3, 8.5, 0.5)]\n",
    "seeds = [0, 1, 11, 111, 42]\n",
    "all_Ns = []\n",
    "for seed in seeds:\n",
    "    print(\"################## SEED = {} ###############\".format(seed))\n",
    "    Ns = []\n",
    "    for d in d_range:\n",
    "        N = binary_sweep(d, d, int(0.5*d*d), [1000.], RNG(seed))\n",
    "        print(N)\n",
    "        Ns.append(N)\n",
    "    all_Ns.append(Ns)\n",
    "    \n",
    "all_Ns = np.array(all_Ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('style.mplstyle')\n",
    "fig, ax = plt.subplots(figsize = (5, 4))\n",
    "d_range = [int(2**i) for i in jnp.arange(3, 8.5, 0.5)]\n",
    "\n",
    "plt.scatter([d**2 for d in d_range], all_Ns.mean(axis=0), marker = 'o', color = 'b')\n",
    "plt.fill_between([d**2 for d in d_range], all_Ns.min(axis=0), all_Ns.max(axis=0), color = 'b', alpha = 0.5)\n",
    "\n",
    "N_ends = [20, 6000]\n",
    "C = 1.3\n",
    "plt.plot([C*N_ends[0]*jnp.log(N_ends[0]), C*N_ends[1]*jnp.log(N_ends[1])], N_ends, linestyle = 'dashed', color = 'gray', label=r'$N\\log N \\propto d^2$')\n",
    "\n",
    "plt.yscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(r'$d^2$')\n",
    "plt.ylabel(r'$N$')\n",
    "plt.title(\"Linear Associative Memory\")\n",
    "plt.legend()\n",
    "plt.savefig(\"AM_rescale.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
